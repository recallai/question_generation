{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114d254b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T21:22:33.983979Z",
     "start_time": "2021-09-24T21:22:32.309978Z"
    }
   },
   "outputs": [],
   "source": [
    "from pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9f4007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T21:58:50.755547Z",
     "start_time": "2021-09-24T21:58:33.034996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n",
    "nlp = pipeline(\"question-generation\")\n",
    "text3 = \"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.\"\n",
    "res = nlp.extract_highlights(text3)\n",
    "#questions = nlp(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf3b5cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T21:54:57.058051Z",
     "start_time": "2021-09-24T21:54:57.055641Z"
    }
   },
   "outputs": [],
   "source": [
    "sent,ans = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66374c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T21:48:06.561399Z",
     "start_time": "2021-09-24T21:48:06.554414Z"
    }
   },
   "outputs": [],
   "source": [
    "def color_choosing(colour, text):\n",
    "    if colour == \"black\":\n",
    "        return \"\\033[1;40m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"red\":\n",
    "        return \"\\033[1;41m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"green\":\n",
    "        return \"\\033[1;42m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"yellow\":\n",
    "        return \"\\033[1;43m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"blue\":\n",
    "        return \"\\033[1;44m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"magenta\":\n",
    "        return \"\\033[1;45m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"cyan\":\n",
    "        return \"\\033[1;46m\" + str(text) + \"\\033[1;m\"\n",
    "    if colour == \"gray\":\n",
    "        return \"\\033[1;47m\" + str(text) + \"\\033[1;m\"\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b85457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a benchmark '],\n",
       " ['817 '],\n",
       " ['false '],\n",
       " ['imitating human texts '],\n",
       " ['T5 '],\n",
       " ['94% '],\n",
       " ['popular misconceptions '],\n",
       " ['The largest models '],\n",
       " ['17% '],\n",
       " ['performance improves with model size '],\n",
       " ['training distribution '],\n",
       " ['scaling up models alone ']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391cb632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T21:54:57.066160Z",
     "start_time": "2021-09-24T21:54:57.059937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose \u001b[1;43ma benchmark\u001b[1;m to measure whether a language model is truthful in generating answers to questions.\n",
      "The benchmark comprises \u001b[1;43m817\u001b[1;m questions that span 38 categories, including health, law, finance and politics.\n",
      "We crafted questions that some humans would answer \u001b[1;43mfalse\u001b[1;mly due to a false belief or misconception.\n",
      "To perform well, models must avoid generating false answers learned from \u001b[1;43mimitating human texts\u001b[1;m.\n",
      "We tested GPT-3, GPT-Neo/J, GPT-2 and a \u001b[1;43mT5\u001b[1;m-based model.\n",
      "The best model was truthful on 58% of questions, while human performance was \u001b[1;43m94%\u001b[1;m.\n",
      "Models generated many false answers that mimic \u001b[1;43mpopular misconceptions\u001b[1;m and have the potential to deceive humans.\n",
      "\u001b[1;43mThe largest models\u001b[1;m were generally the least truthful.\n",
      "For example, the 6B-parameter GPT-J model was \u001b[1;43m17%\u001b[1;m less truthful than its 125M-parameter counterpart.\n",
      "This contrasts with other NLP tasks, where \u001b[1;43mperformance improves with model size\u001b[1;m.\n",
      "However, this result is expected if false answers are learned from the \u001b[1;43mtraining distribution\u001b[1;m.\n",
      "We suggest that \u001b[1;43mscaling up models alone\u001b[1;m is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['We propose \\x1b[1;43ma benchmark\\x1b[1;m to measure whether a language model is truthful in generating answers to questions.',\n",
       " 'The benchmark comprises \\x1b[1;43m817\\x1b[1;m questions that span 38 categories, including health, law, finance and politics.',\n",
       " 'We crafted questions that some humans would answer \\x1b[1;43mfalse\\x1b[1;mly due to a false belief or misconception.',\n",
       " 'To perform well, models must avoid generating false answers learned from \\x1b[1;43mimitating human texts\\x1b[1;m.',\n",
       " 'We tested GPT-3, GPT-Neo/J, GPT-2 and a \\x1b[1;43mT5\\x1b[1;m-based model.',\n",
       " 'The best model was truthful on 58% of questions, while human performance was \\x1b[1;43m94%\\x1b[1;m.',\n",
       " 'Models generated many false answers that mimic \\x1b[1;43mpopular misconceptions\\x1b[1;m and have the potential to deceive humans.',\n",
       " '\\x1b[1;43mThe largest models\\x1b[1;m were generally the least truthful.',\n",
       " 'For example, the 6B-parameter GPT-J model was \\x1b[1;43m17%\\x1b[1;m less truthful than its 125M-parameter counterpart.',\n",
       " 'This contrasts with other NLP tasks, where \\x1b[1;43mperformance improves with model size\\x1b[1;m.',\n",
       " 'However, this result is expected if false answers are learned from the \\x1b[1;43mtraining distribution\\x1b[1;m.',\n",
       " 'We suggest that \\x1b[1;43mscaling up models alone\\x1b[1;m is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight(sentences,highlights,colour='cyan'):\n",
    "    sentences = sentences[:]\n",
    "    for idx,sent_match in enumerate(highlights):\n",
    "        current_pos_sent = 0\n",
    "        for match in sent_match:\n",
    "            token = match.replace('<pad>','').strip()\n",
    "            token_highlighted = color_choosing(colour,token)\n",
    "            \n",
    "            #replace in text\n",
    "            begin_pos = sentences[idx][current_pos_sent:].find(token)\n",
    "            \n",
    "            sentences[idx] = sentences[idx][:current_pos_sent + begin_pos] + \\\n",
    "                                token_highlighted + \\\n",
    "                             sentences[idx][current_pos_sent + begin_pos + len(token):]\n",
    "            print(sentences[idx])\n",
    "            current_pos_sent += begin_pos + len(token)\n",
    "    return sentences\n",
    "            \n",
    "            \n",
    "highlight(sent,ans,\"yellow\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ce900e38693e32eb74b28fff426324061722822d4fccdca33c6416620e7890a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
